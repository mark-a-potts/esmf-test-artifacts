--------------------------------------------------------------------------
WARNING: Open MPI will create a shared memory backing file in a
directory that appears to be mounted on a network filesystem.
Creating the shared memory backup file on a network file system, such
as NFS or Lustre is not recommended -- it may cause excessive network
traffic to your file servers and/or cause shared memory traffic in
Open MPI to be much slower than expected.

You may want to check what the typical temporary directory is on your
node.  Possible sources of the location of this temporary directory
include the $TEMPDIR, $TEMP, and $TMP environment variables.

Note, too, that system administrators can set a list of filesystems
where Open MPI is disallowed from creating temporary files by setting
the MCA parameter "orte_no_session_dir".

  Local host: r2i1n7
  Filename:   /p/work1/tmp/mpotts/openmpi-sessions-10547@r2i1n7_0/35421/1/2/vader_segment.r2i1n7.2

You can set the MCA paramter shmem_mmap_enable_nfs_warning to 0 to
disable this message.
--------------------------------------------------------------------------
Beginning Test, file ESMC_DistGridUTest.C, line 44
Beginning Test, file ESMC_DistGridUTest.C, line 44
NUMBER_OF_PROCESSORS 4
PASS Set up minIndex, ESMC_DistGridUTest.C, line 54
PASS Set up maxIndex, ESMC_DistGridUTest.C, line 65
NUMBER_OF_PROCESSORS 4
PASS Set up minIndex, ESMC_DistGridUTest.C, line 54
PASS Set up maxIndex, ESMC_DistGridUTest.C, line 65
Beginning Test, file ESMC_DistGridUTest.C, line 44
PASS Create 5 x 10 ESMC_DistGrid object, ESMC_DistGridUTest.C, line 73
PASS Create 5 x 10 ESMC_DistGrid object, ESMC_DistGridUTest.C, line 73
NUMBER_OF_PROCESSORS 4
--- ESMCI::DistGrid::print start ---
indexTK: Integer*4
--- ESMCI::DistGrid::print start ---
indexTK: Integer*4
dimCount = 2
tileCount = 1
elementCountPTile: 50 
regDecomp = YES
tileListPDe: 1 1 1 1 
elementCountPDe: 20 10 10 10 
contigFlagPDimPDe (dims separated by / ):
 for DE 0: 1 / 1 / 
 for DE 1: 1 / 1 / 
 for DE 2: 1 / 1 / 
PASS Set up minIndex, ESMC_DistGridUTest.C, line 54
PASS Set up maxIndex, ESMC_DistGridUTest.C, line 65
dimCount = 2
tileCount = 1
elementCountPTile: 50 
regDecomp = YES
tileListPDe: 1 1 1 1 
elementCountPDe: 20 10 10 10 
contigFlagPDimPDe (dims separated by / ):
 for DE 0: 1 / 1 / 
 for DE 1: 1 / 1 / 
 for DE 2: 1 / 1 / 
 for DE 3: 1 / 1 / 
(min,max)IndexPDimPDe (dims separated by / ):
 for DE 0: (1, 2) / (1, 10) / 
 for DE 1: (3, 3) / (1, 10) / 
 for DE 2: (4, 4) / (1, 10) / 
 for DE 3: (5, 5) / (1, 10) / 
indexListPDimPLocalDe (dims separated by / ):
 for localDE 0 - DE 0:  (1, 2) / (1, 2, 3, 4, 5, 6, 7, 8, 9, 10) /
diffCollocationCount = 1
 for DE 3: 1 / 1 / 
(min,max)IndexPDimPDe (dims separated by / ):
 for DE 0: (1, 2) / (1, 10) / 
 for DE 1: (3, 3) / (1, 10) / 
 for DE 2: (4, 4) / (1, 10) / 
 for DE 3: (5, 5) / (1, 10) / 
indexListPDimPLocalDe (dims separated by / ):
 for localDE 0 - DE 2:  (4) / (1, 2, 3, 4, 5, 6, 7, 8, 9, 10) /
diffCollocationCount = 1
collocationPDim:	1 / 1
arbSeqIndexListPCollPLocalDe:
 for collocation 1, localDE 0 - DE 2 -  elementCountPCollPLocalDe 10:  default
connectionCount = 0
~ lower class' values ~
deCount = 4
Member on this PET appears to be an actual member.
DistGrid-VM: localPet = 2, CurrentVM: localPet = 2
DistGrid-VM: petCount = 4, CurrentVM: petCount = 4
--- ESMCI::DistGrid::print end ---
collocationPDim:	1 / 1
arbSeqIndexListPCollPLocalDe:
 for collocation 1, localDE 0 - DE 0 -  elementCountPCollPLocalDe 20:  default
connectionCount = 0
~ lower class' values ~
deCount = 4
Member on this PET appears to be an actual member.
DistGrid-VM: localPet = 0, CurrentVM: localPet = 0
DistGrid-VM: petCount = 4, CurrentVM: petCount = 4
--- ESMCI::DistGrid::print end ---
Beginning Test, file ESMC_DistGridUTest.C, line 44
NUMBER_OF_PROCESSORS 4
PASS Set up minIndex, ESMC_DistGridUTest.C, line 54
PASS Set up maxIndex, ESMC_DistGridUTest.C, line 65
PASS Print ESMC_DistGrid object, ESMC_DistGridUTest.C, line 85
PASS Print ESMC_DistGrid object, ESMC_DistGridUTest.C, line 85
PASS Destroy ESMC_DistGrid object, ESMC_DistGridUTest.C, line 93
PASS Destroy ESMC_DistGrid object, ESMC_DistGridUTest.C, line 93
PASS Create 5 x 10 ESMC_DistGrid object, ESMC_DistGridUTest.C, line 73
PASS Print destroyed ESMC_DistGrid object, ESMC_DistGridUTest.C, line 101
PASS Print destroyed ESMC_DistGrid object, ESMC_DistGridUTest.C, line 101
Ending Test, file ESMC_DistGridUTest.C, line 105
Ending Test, file ESMC_DistGridUTest.C, line 105
--- ESMCI::DistGrid::print start ---
indexTK: Integer*4
dimCount = 2
tileCount = 1
elementCountPTile: 50 
regDecomp = YES
tileListPDe: 1 1 1 1 
elementCountPDe: 20 10 10 10 
contigFlagPDimPDe (dims separated by / ):
 for DE 0: 1 / 1 / 
 for DE 1: 1 / 1 / 
 for DE 2: 1 / 1 / 
 for DE 3: 1 / 1 / 
(min,max)IndexPDimPDe (dims separated by / ):
 for DE 0: (1, 2) / (1, 10) / 
 for DE 1: (3, 3) / (1, 10) / 
 for DE 2: (4, 4) / (1, 10) / 
 for DE 3: (5, 5) / (1, 10) / 
indexListPDimPLocalDe (dims separated by / ):
 for localDE 0 - DE 1:  (3) / (1, 2, 3, 4, 5, 6, 7, 8, 9, 10) /
diffCollocationCount = 1
--- ESMCI::DistGrid::print start ---
indexTK: Integer*4
dimCount = 2
PASS Create 5 x 10 ESMC_DistGrid object, ESMC_DistGridUTest.C, line 73
collocationPDim:	1 / 1
arbSeqIndexListPCollPLocalDe:
 for collocation 1, localDE 0 - DE 1 -  elementCountPCollPLocalDe 10:  default
connectionCount = 0
~ lower class' values ~
deCount = 4
Member on this PET appears to be an actual member.
DistGrid-VM: localPet = 1, CurrentVM: localPet = 1
DistGrid-VM: petCount = 4, CurrentVM: petCount = 4
--- ESMCI::DistGrid::print end ---
tileCount = 1
elementCountPTile: 50 
regDecomp = YES
tileListPDe: 1 1 1 1 
elementCountPDe: 20 10 10 10 
contigFlagPDimPDe (dims separated by / ):
 for DE 0: 1 / 1 / 
 for DE 1: 1 / 1 / 
 for DE 2: 1 / 1 / 
 for DE 3: 1 / 1 / 
(min,max)IndexPDimPDe (dims separated by / ):
PASS Print ESMC_DistGrid object, ESMC_DistGridUTest.C, line 85
PASS Destroy ESMC_DistGrid object, ESMC_DistGridUTest.C, line 93
 for DE 0: (1, 2) / (1, 10) / 
 for DE 1: (3, 3) / (1, 10) / 
 for DE 2: (4, 4) / (1, 10) / 
 for DE 3: (5, 5) / (1, 10) / 
indexListPDimPLocalDe (dims separated by / ):
 for localDE 0 - DE 3:  (5) / (1, 2, 3, 4, 5, 6, 7, 8, 9, 10) /
diffCollocationCount = 1
collocationPDim:	1 / 1
arbSeqIndexListPCollPLocalDe:
 for collocation 1, localDE 0 - DE 3 -  elementCountPCollPLocalDe 10:  default
connectionCount = 0
~ lower class' values ~
deCount = 4
Member on this PET appears to be an actual member.
DistGrid-VM: localPet = 3, CurrentVM: localPet = 3
DistGrid-VM: petCount = 4, CurrentVM: petCount = 4
--- ESMCI::DistGrid::print end ---
PASS Print destroyed ESMC_DistGrid object, ESMC_DistGridUTest.C, line 101
PASS Print ESMC_DistGrid object, ESMC_DistGridUTest.C, line 85
PASS Destroy ESMC_DistGrid object, ESMC_DistGridUTest.C, line 93
Ending Test, file ESMC_DistGridUTest.C, line 105
PASS Print destroyed ESMC_DistGrid object, ESMC_DistGridUTest.C, line 101
Ending Test, file ESMC_DistGridUTest.C, line 105
 PET 0 Test Elapsed Time  52.152000 msec.
 PET 1 Test Elapsed Time  51.911000 msec.
 PET 2 Test Elapsed Time  53.439000 msec.
 PET 3 Test Elapsed Time  53.337000 msec.
[r2i1n7:100881] 5 more processes have sent help message help-opal-shmem-mmap.txt / mmap on nfs
[r2i1n7:100881] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
